import torch
import json
import numpy as np
import torch.nn.functional as F
import matplotlib.pyplot as plt

class LRScheduler():
    """
    Learning rate scheduler. If the validation loss does not decrease for the 
    given number of `patience` epochs, then the learning rate will decrease by
    by given `factor`.
    """
    def __init__(
        self, optimizer, patience=5, min_lr=1e-6, factor=0.5
    ):
        """
        new_lr = old_lr * factor

        :param optimizer: the optimizer we are using
        :param patience: how many epochs to wait before updating the lr
        :param min_lr: least lr value to reduce to while updating
        :param factor: factor by which the lr should be updated
        """
        self.optimizer = optimizer
        self.patience = patience
        self.min_lr = min_lr
        self.factor = factor

        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( 
                self.optimizer,
                mode='min',
                patience=self.patience,
                factor=self.factor,
                min_lr=self.min_lr,
                verbose=True
            )

    def __call__(self, val_loss):
        self.lr_scheduler.step(val_loss)


class EarlyStopping():
    """
    Early stopping to stop the training when the loss does not improve after
    certain epochs.
    """
    def __init__(self, patience=10, min_delta=0):
        """
        :param patience: how many epochs to wait before stopping when loss is
               not improving
        :param min_delta: minimum difference between new loss and old loss for
               new loss to be considered as an improvement
        """
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.current_is_best = False
    def __call__(self, val_loss):
        if self.best_loss == None:
            self.best_loss = val_loss
        elif self.best_loss - val_loss > self.min_delta:
            self.best_loss = val_loss
            self.current_is_best = True
            # reset counter if validation loss improves
            self.counter = 0
        elif self.best_loss - val_loss < self.min_delta:
            self.counter += 1
            self.current_is_best = False
            print(f"INFO: Early stopping counter {self.counter} of {self.patience}")
            if self.counter >= self.patience:
                print('INFO: Early stopping')
                self.early_stop = True

def save_argparse(root, args, filename):
    print('working on %s directory...' % root)
    with open('%s%s.txt' % (root, filename), 'w') as f:
        json.dump(args.__dict__, f, indent=2)

def encoding_normals(pred):
    # pred in (B, C, H, W)
    b = torch.round((0.5*pred[:,0]+0.5)*255)
    g = torch.round((0.5*pred[:,1]+0.5)*255)
    r = torch.round((0.5*pred[:,2]+0.5)*255)
    # norm_pred = (pred - torch.min(pred)) / (torch,max(pred) - torch.min(pred))
    return torch.stack((b,g,r),1)
    # return norm_pred

def decoding_normals(img):
    z = img[:,:,0]/255*2-1
    y = img[:,:,1]/255*2-1
    x = img[:,:,2]/255*2-1
    return np.dstack((z,y,x))

def norm_depth(img):
    img = img*2-1
    return img

def inv_norm_depth(img):
    img = img*0.5+0.5
    return img*255

def calc_normal_uncertainty(pred, gt):
    B,C,H,W = gt.shape
    prediction_error = torch.cosine_similarity(pred, gt, dim=1)
    prediction_error = torch.clamp(prediction_error, min=-1.0, max=1.0)

    prediction_error[prediction_error==0]=1
    E = torch.acos(prediction_error)
    E = torch.rad2deg(E)
    E[E<0.5]=0

    normalized_diff = F.normalize(E.reshape(B, H*W), p = 2, dim = 1)
    return normalized_diff.reshape(B,1,H,W), E